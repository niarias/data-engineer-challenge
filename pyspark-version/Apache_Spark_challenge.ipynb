{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PYBY89Gj0pe0",
        "7C6ggARAkMti",
        "Ba7RdydZH9Wk",
        "zDc_H8MD6HzW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies"
      ],
      "metadata": {
        "id": "WSqeg8Pv63eo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ1Z0LwmH2ls",
        "outputId": "9d875796-07b1-4436-98e9-9a620a52fd2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,substring,trim,to_timestamp,lit\n",
        "from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType\n",
        "import json\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "import uuid\n"
      ],
      "metadata": {
        "id": "-vwnyGZ7Iata"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"test_pyspark\").getOrCreate()"
      ],
      "metadata": {
        "id": "nJ5ULKlYIxkO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load utils"
      ],
      "metadata": {
        "id": "PYBY89Gj0pe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json_file(path):\n",
        "    with open(path, 'r') as file:\n",
        "        config = json.load(file)\n",
        "    return config\n",
        "\n",
        "\n",
        "def array_to_colspecs(arr):\n",
        "    start_pos = 0\n",
        "    colspecs = []\n",
        "\n",
        "    for width in arr:\n",
        "        end_pos = start_pos + width\n",
        "        colspecs.append((start_pos, end_pos))\n",
        "        start_pos = end_pos\n",
        "\n",
        "    return colspecs\n",
        "\n",
        "def generate_file_schema(columns_name,columns_type):\n",
        "  schema_fields = []\n",
        "  for name, dtype in zip(config[\"metadata\"][\"columns_name\"], config[\"metadata\"][\"columns_type\"]):\n",
        "      if dtype == \"string\":\n",
        "          schema_fields.append(StructField(name, StringType(), True))\n",
        "      elif dtype == \"timestamp\":\n",
        "          schema_fields.append(StructField(name, TimestampType(), True))\n",
        "      elif dtype == \"float\":\n",
        "          schema_fields.append(StructField(name, FloatType(), True))\n",
        "\n",
        "  schema = StructType(schema_fields)\n",
        "  return schema\n",
        "\n",
        "# Function to generate column expressions based on widths\n",
        "def generate_column_exprs(widths, names):\n",
        "    exprs = []\n",
        "    startPos = 1\n",
        "    for width, name in zip(widths, names):\n",
        "        exprs.append(substring(col(\"value\"), startPos, width).alias(name))\n",
        "        startPos += width\n",
        "    return exprs\n",
        "\n",
        "def verify_dict(dict_to_check,required_fields):\n",
        "    for field in required_fields:\n",
        "        if field not in dict_to_check:\n",
        "            raise ValueError(f\"Field {field} is required in the config file\")"
      ],
      "metadata": {
        "id": "8wA09wwizXsn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading config"
      ],
      "metadata": {
        "id": "eLN3dF3x6fd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Fixed width\n",
        "#config_path = \"/content/config_fixed_width.json\"\n",
        "## Delimiter\n",
        "config_path = \"/content/config.json\"\n",
        "config = load_json_file(config_path)"
      ],
      "metadata": {
        "id": "_8NsTN8W4ZlC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Br9m0frN61u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strategies"
      ],
      "metadata": {
        "id": "1f9cfD2dH035"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Abstract"
      ],
      "metadata": {
        "id": "7C6ggARAkMti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderStrategy(ABC):\n",
        "    @abstractmethod  #This method will validate the config file\n",
        "    def load_data(self, config):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod    #This method will load the data from a delimited file\n",
        "    def validateConfig(self, config):\n",
        "        pass"
      ],
      "metadata": {
        "id": "GAVYoV6LkPQj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixed_width strategy"
      ],
      "metadata": {
        "id": "Ba7RdydZH9Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FixedWidthFileLoader(DataLoaderStrategy):\n",
        "  def validateConfig(self, config):\n",
        "        required_fields = ['origin_path', 'final_path', 'metadata']\n",
        "        verify_dict(config,required_fields)\n",
        "\n",
        "        metadata_required_fields = ['columns_widths', 'columns_name','columns_type']\n",
        "        verify_dict(config['metadata'],metadata_required_fields)\n",
        "\n",
        "  def load_data(self, config):\n",
        "    self.validateConfig(config)\n",
        "    df = spark.read.text(config[\"origin_path\"])\n",
        "    # Use the metadata to parse the DataFrame columns\n",
        "    column_exprs = generate_column_exprs(config[\"metadata\"][\"columns_widths\"], config[\"metadata\"][\"columns_name\"])\n",
        "    df = df.select(*column_exprs)\n",
        "    # Casting the columns to their specified data types based on the JSON metadata\n",
        "    for name, dtype in zip(config[\"metadata\"][\"columns_name\"], config[\"metadata\"][\"columns_type\"]):\n",
        "        if dtype == \"float\":\n",
        "            df = df.withColumn(name, col(name).cast(FloatType()))\n",
        "        if(dtype == \"string\"):\n",
        "            df = df.withColumn(name, trim(col(name)))\n",
        "        if(dtype == 'timestamp'):\n",
        "            df = df.withColumn(name, trim(col(name)))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "jW0LLUrb4th2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delimited Parser\n"
      ],
      "metadata": {
        "id": "nQmfk58lIj6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DelimitedFileLoader(DataLoaderStrategy):\n",
        "   def validateConfig(self, config):\n",
        "        required_fields = ['origin_path', 'final_path', 'metadata']\n",
        "        verify_dict(config,required_fields)\n",
        "\n",
        "        metadata_required_fields = ['delimiter']\n",
        "        verify_dict(config['metadata'],metadata_required_fields)\n",
        "\n",
        "\n",
        "   def load_data(self, config):\n",
        "      self.validateConfig(config)\n",
        "      df = spark.read.csv(config['origin_path'], header=True, inferSchema=True, sep=config['metadata']['delimiter'])\n",
        "      return df"
      ],
      "metadata": {
        "id": "6xhfH-cxI0jR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S3ToFJCXomdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Strategy\n"
      ],
      "metadata": {
        "id": "6Bpd7ro9onhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy_map = {\n",
        "    'delimited': DelimitedFileLoader(),\n",
        "    'fixed_width': FixedWidthFileLoader()\n",
        "}\n",
        "\n",
        "\n",
        "def get_strategy(strategy_type):\n",
        "    strategy = strategy_map[strategy_type]\n",
        "    if not strategy:\n",
        "        raise ValueError(f\"Invalid strategy: {strategy_type}\")\n",
        "    return strategy"
      ],
      "metadata": {
        "id": "zg88SY2Koqfn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read DF"
      ],
      "metadata": {
        "id": "n0aKajIUpxNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = get_strategy(config['metadata']['type'])\n",
        "df = strategy.load_data(config)"
      ],
      "metadata": {
        "id": "0EDZqs-Pp3Ab"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Aditional Data"
      ],
      "metadata": {
        "id": "y2xKyoCgFOWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'additional_data' in config['metadata']:\n",
        "    for col, value in config['metadata']['additional_data'].items():\n",
        "        df = df.withColumn(col, lit(value))"
      ],
      "metadata": {
        "id": "oll4jystFRdK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C0tRkxsFySQ",
        "outputId": "aa0aed61-1ace-49a2-ff4b-53b82166fc28"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(stock='A', transaction_date='02-Jun-2012', open_price=34.93, close_price=34.93, max_price=34.93, min_price=34.93, variation=0, partition_date='2024-04-05')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Data"
      ],
      "metadata": {
        "id": "zDc_H8MD6HzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if 'partition_by' is in your configuration and write the DataFrame to Parquet accordingly\n",
        "full_path = f\"{config['final_path']}/{uuid.uuid4()}\"\n",
        "if 'partition_by' in config:\n",
        "    # Write DataFrame to Parquet with partitioning\n",
        "    df.write.partitionBy(config['partition_by']).parquet(full_path)\n",
        "else:\n",
        "    # Write DataFrame to Parquet without partitioning\n",
        "    df.write.parquet(full_path)"
      ],
      "metadata": {
        "id": "BBZQXbT56F-b"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}